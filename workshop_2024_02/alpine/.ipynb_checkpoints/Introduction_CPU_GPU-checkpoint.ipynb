{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matrix_multiply(A, B):\n",
    "    n = len(A)\n",
    "    p = len(B[0])\n",
    "    m = len(B)\n",
    "    result = [[0] * p for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(p):\n",
    "            for k in range(m):\n",
    "                result[i][j] += A[i][k] * B[k][j]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matrix_multiply_np(A, B,Reps):\n",
    "    C=np.dot(A, B)\n",
    "    for i in range(Reps):\n",
    "        C=np.dot(A, C)\n",
    "    return C    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matrix_multiply_torch(A, B, Reps):\n",
    "    # Check if CUDA (GPU support) is available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    A_torch = torch.tensor(A, dtype=torch.float64).to(device)\n",
    "    B_torch = torch.tensor(B, dtype=torch.float64).to(device)\n",
    "    C_torch = torch.matmul(A_torch, B_torch)  # Perform the first multiplication on the GPU\n",
    "\n",
    "    for i in range(Reps):  # Subtract 1 because the first multiplication is already done\n",
    "        C_torch = torch.matmul(A_torch, C_torch)  # Further multiplications on the GPU\n",
    "\n",
    "    C = C_torch.cpu().numpy()  # Only move the final result to CPU and convert to NumPy\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n",
      "Plain Python time: 19.04476809501648\n",
      "NumPy time: 0.035514116287231445\n",
      "PyTorch (GPU) time: 0.9726755619049072\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Generate random matrices\n",
    "n = 512\n",
    "Reps=20\n",
    "A = np.random.rand(n, n).astype(np.float64)\n",
    "B = np.random.rand(n, n).astype(np.float64)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device is:\",device)\n",
    "\n",
    "#Measure performance of plain Python\n",
    "start_time = time.time()\n",
    "result_python = matrix_multiply(A.tolist(), B.tolist())\n",
    "print(\"Plain Python time:\", time.time() - start_time)\n",
    "\n",
    "# Measure performance of NumPy\n",
    "start_time = time.time()\n",
    "result_np = matrix_multiply_np(A, B,Reps)\n",
    "print(\"NumPy time:\", time.time() - start_time)\n",
    "\n",
    "# Measure performance of PyTorch with GPU\n",
    "start_time = time.time()\n",
    "result_torch = matrix_multiply_torch(A, B,Reps)\n",
    "print(\"PyTorch (GPU) time:\", time.time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using timeit to properly benchmark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n",
      "numpy  1 0.004148013099984383\n",
      "Torch  1 0.01723056480004743\n",
      "numpy  11 0.029338015200119118\n",
      "Torch  11 0.03934437269999762\n",
      "numpy  21 0.05579080410025199\n",
      "Torch  21 0.05489569599994866\n",
      "numpy  31 0.0698926749999373\n",
      "Torch  31 0.07817690780029807\n",
      "numpy  41 0.09126125490001868\n",
      "Torch  41 0.09182877209968865\n",
      "numpy  51 0.11634553100011544\n",
      "Torch  51 0.12179413070007286\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use the 'Agg' backend for file creation\n",
    "import torch\n",
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device is:\",device)\n",
    "\n",
    "n = 512\n",
    "A = np.random.rand(n, n).astype(np.float64)\n",
    "B = np.random.rand(n, n).astype(np.float64)\n",
    "reps_range = range(1, 61, 10)\n",
    "numpy_times = []\n",
    "torch_times = []\n",
    "number = 10\n",
    "\n",
    "for Reps in reps_range:\n",
    "    # Time NumPy\n",
    "    numpy_time = timeit.timeit('matrix_multiply_np(A, B, Reps)', globals=globals(), number=number)/number\n",
    "    print('numpy ',Reps, numpy_time)\n",
    "    numpy_times.append(numpy_time)\n",
    "\n",
    "    # Time PyTorch\n",
    "    torch_time = timeit.timeit('matrix_multiply_torch(A, B,  Reps)', globals=globals(), number=number)/number\n",
    "    print('Torch ',Reps, torch_time)\n",
    "    torch_times.append(torch_time)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reps_range, numpy_times, label='NumPy', marker='o')\n",
    "plt.plot(reps_range, torch_times, label='PyTorch', marker='x')\n",
    "plt.xlabel('Number of Repetitions (Reps)')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Performance Comparison: NumPy vs. PyTorch GPU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.draw()\n",
    "plt.savefig('reps_docker.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using more GPU intensive calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 256x256\n",
      "NumPy Duration: 0.0005094526000902988\n",
      "Torch GPU Duration:  0.002942051300124149  \n",
      "\n",
      "Size: 512x512\n",
      "NumPy Duration: 0.0028534634999232366\n",
      "Torch GPU Duration:  0.014748130799853243  \n",
      "\n",
      "Size: 1024x1024\n",
      "NumPy Duration: 0.008574329299881356\n",
      "Torch GPU Duration:  0.025905234699894208  \n",
      "\n",
      "Size: 2048x2048\n",
      "NumPy Duration: 0.0890007317000709\n",
      "Torch GPU Duration:  0.14240206200011016  \n",
      "\n",
      "Size: 4096x4096\n",
      "NumPy Duration: 0.7102244009998685\n",
      "Torch GPU Duration:  0.9560921682998014  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34093/1238104374.py:50: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use the 'Agg' backend for file creation\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def multiply_numpy(A, B):\n",
    "    return np.dot(A, B)\n",
    "\n",
    "def multiply_torch(A, B):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    A_torch = torch.tensor(A, dtype=torch.float64).to(device)\n",
    "    B_torch = torch.tensor(B, dtype=torch.float64).to(device)\n",
    "    return torch.matmul(A_torch, B_torch).cpu().numpy()\n",
    "\n",
    "# Sizes of the matrices to test\n",
    "sizes = [256, 512, 2**10, 2**11, 2**12]\n",
    "numpy_times = []\n",
    "torch_times = []\n",
    "\n",
    "number=10\n",
    "for size in sizes:\n",
    "    A = np.random.rand(size, size).astype(np.float64)\n",
    "    B = np.random.rand(size, size).astype(np.float64)\n",
    "\n",
    "    # Time NumPy\n",
    "    numpy_duration = timeit.timeit('multiply_numpy(A, B)', globals=globals(), number=number)/number\n",
    "    numpy_times.append(numpy_duration)\n",
    "    print(f\"Size: {size}x{size}\")\n",
    "    print(\"NumPy Duration:\", numpy_duration)\n",
    "\n",
    "    # Time PyTorch with GPU\n",
    "    torch_duration = timeit.timeit('multiply_torch(A, B)', globals=globals(), number=number)/number\n",
    "    torch_times.append(torch_duration)\n",
    "    print(\"Torch GPU Duration: \", torch_duration,' \\n')\n",
    "\n",
    "       \n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sizes, numpy_times, label='NumPy', marker='o')\n",
    "plt.plot(sizes, torch_times, label='PyTorch GPU', marker='x')\n",
    "plt.xlabel('Matrix Size')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Performance Comparison: NumPy vs. PyTorch GPU')\n",
    "plt.yscale('log')  # Set the y-axis to logarithmic scale\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")  # Enable grid for major and minor ticks\n",
    "plt.draw()\n",
    "plt.savefig('sizes_docker.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different GPU optimized functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Duration: 1.6110794904001522\n",
      "Torch GPU Duration: 0.8093623316999583\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import timeit\n",
    "\n",
    "# Example image loading and conversion to numpy\n",
    "image = Image.open('Kinkade.jpg').convert('RGB')  # Ensure image is in RGB\n",
    "image_np = np.array(image).astype(np.float32)  # Convert to float for better handling in PyTorch\n",
    "image_torch = torch.tensor(image_np.transpose(2, 0, 1)).unsqueeze(0).to('cuda')  # Convert to Tensor and move to GPU\n",
    "\n",
    "# Define the target size as a tuple (width, height)\n",
    "size = (512*20, 512*20)  # Define the size as a tuple\n",
    "\n",
    "def numpy_resize(image_np, size):\n",
    "    return np.array(Image.fromarray(image_np.astype(np.uint8)).resize(size))\n",
    "\n",
    "def torch_resize(image_torch, size):\n",
    "    # size needs to be (height, width) for PyTorch, reverse the tuple\n",
    "    torch_size = (size[1], size[0])\n",
    "    return torch.nn.functional.interpolate(image_torch, size=torch_size, mode='bilinear', align_corners=False).cpu()\n",
    "\n",
    "# Timing the functions using timeit\n",
    "number = 10  # Number of iterations for timing\n",
    "\n",
    "# Time NumPy\n",
    "numpy_time = timeit.timeit('numpy_resize(image_np, size)', globals=globals(), number=number) / number\n",
    "\n",
    "# Time PyTorch\n",
    "torch_time = timeit.timeit('torch_resize(image_torch, size)', globals=globals(), number=number) / number\n",
    "\n",
    "print(\"NumPy Duration:\", numpy_time)\n",
    "print(\"Torch GPU Duration:\", torch_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions not optimized for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a large array and summing it\n",
    "import timeit\n",
    "import numpy as np\n",
    "import torch\n",
    "large_data = np.random.rand(10**8)  # 10 million elements\n",
    "\n",
    "\n",
    "def sum_numpy(data):\n",
    "    return np.sum(data)\n",
    "\n",
    "def sum_torch(data):\n",
    "    tensor = torch.tensor(data).cuda()\n",
    "    return torch.sum(tensor).cpu()\n",
    "\n",
    "number=10\n",
    "# Timing the sum operations\n",
    "numpy_time = timeit.timeit('sum_numpy(large_data)', globals=globals(), number=number) / number\n",
    "\n",
    "\n",
    "torch_time = timeit.timeit('sum_torch(large_data)', globals=globals(), number=number) / number\n",
    "\n",
    "\n",
    "print(\"NumPy time:\", numpy_time)\n",
    "print(\"Torch time:\", torch_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
