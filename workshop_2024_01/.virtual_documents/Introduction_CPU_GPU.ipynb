def matrix_multiply(A, B):
    n = len(A)
    p = len(B[0])
    m = len(B)
    result = [[0] * p for _ in range(n)]
    for i in range(n):
        for j in range(p):
            for k in range(m):
                result[i][j] += A[i][k] * B[k][j]
    return result



def matrix_multiply_np(A, B,Reps):
    C=np.dot(A, B)
    for i in range(Reps):
        C=np.dot(A, C)
    return C    



def matrix_power_np(A, Reps):
    B=np.power(A,Reps)
    return B    


def matrix_multiply_torch(A, B, Reps):
    A_torch = torch.tensor(A).cuda()
    B_torch = torch.tensor(B).cuda()
    C_torch = torch.matmul(A_torch, B_torch)  # Perform the first multiplication on the GPU

    for i in range(Reps):  # Subtract 1 because the first multiplication is already done
        C_torch = torch.matmul(A_torch, C_torch)  # Further multiplications on the GPU

    C = C_torch.cpu().numpy()  # Only move the final result to CPU and convert to NumPy
    return C



def matrix_power_torch(A, Reps):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    A_torch = torch.tensor(A, dtype=torch.float32, device=device)  # Ensure A is a floating point tensor for matrix power
    result = torch.matrix_power(A_torch, Reps)
    return result.cpu().numpy()


import time

# Generate random matrices
n = 1024
Reps=4000
A = np.random.rand(n, n)
B = np.random.rand(n, n)

# Measure performance of plain Python
#start_time = time.time()
#result_python = matrix_multiply(A.tolist(), B.tolist())
#print("Plain Python time:", time.time() - start_time)

# Measure performance of NumPy
start_time = time.time()
result_np = matrix_multiply_np(A, B,Reps)
print("NumPy time:", time.time() - start_time)

# Measure performance of PyTorch with GPU
start_time = time.time()
result_torch = matrix_multiply_torch(A, B,Reps)
print("PyTorch (GPU) time:", time.time() - start_time)



import torch
import timeit
import numpy as np
import matplotlib.pyplot as plt

if torch.cuda.is_available():
    gpu_message = "PyTorch is using GPU."
else:
    gpu_message = "PyTorch is not using GPU."
n = 2024*3
A = np.random.rand(n, n)
B = np.random.rand(n, n)
reps_range = range(1, 100001, 10000)
numpy_times = []
torch_times = []
number = 10

for Reps in reps_range:
    # Time NumPy
    numpy_time = timeit.timeit('matrix_power_np(A,  Reps)', globals=globals(), number=number)/number
    print('numpy ',Reps, numpy_time)
    numpy_times.append(numpy_time)

    # Time PyTorch
    torch_time = timeit.timeit('matrix_power_torch(A,  Reps)', globals=globals(), number=number)/number
    print('Torch ',Reps, torch_time)
    torch_times.append(torch_time)

plt.figure(figsize=(10, 5))
plt.plot(reps_range, numpy_times, label='NumPy', marker='o')
plt.plot(reps_range, torch_times, label='PyTorch', marker='x')
plt.xlabel('Number of Repetitions (Reps)')
plt.ylabel('Time (seconds)')
plt.title('Performance Comparison: NumPy vs. PyTorch GPU')
plt.legend()
plt.grid(True)
plt.show()



import numpy as np
import torch
import time
import matplotlib.pyplot as plt

def multiply_numpy(A, B):
    return np.dot(A, B)

def multiply_torch(A, B):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    A_torch = torch.tensor(A, dtype=torch.float32).to(device)
    B_torch = torch.tensor(B, dtype=torch.float32).to(device)
    return torch.matmul(A_torch, B_torch).cpu()

# Sizes of the matrices to test
sizes = [256, 512, 1024, 2048, 4096,2**13,2**14]
numpy_times = []
torch_times = []

for size in sizes:
    A = np.random.rand(size, size).astype(np.float32)
    B = np.random.rand(size, size).astype(np.float32)

    # Time NumPy
    start_time = time.time()
    multiply_numpy(A, B)
    numpy_duration = time.time() - start_time
    numpy_times.append(numpy_duration)

    # Time PyTorch with GPU
    start_time = time.time()
    multiply_torch(A, B)
    torch_duration = time.time() - start_time
    torch_times.append(torch_duration)

    print(f"Size: {size}x{size}")
    print("NumPy Duration:", numpy_duration)
    print("Torch GPU Duration:", torch_duration)
    if torch.cuda.is_available():
        print("PyTorch is using GPU.\n")
    else:
        print("PyTorch is not using GPU.\n")

# Plotting the results
plt.figure(figsize=(10, 5))
plt.plot(sizes, numpy_times, label='NumPy', marker='o')
plt.plot(sizes, torch_times, label='PyTorch GPU', marker='x')
plt.xlabel('Matrix Size')
plt.ylabel('Time (seconds)')
plt.title('Performance Comparison: NumPy vs. PyTorch GPU')
plt.yscale('log')  # Set the y-axis to logarithmic scale
plt.legend()
plt.grid(True, which="both", ls="--")  # Enable grid for major and minor ticks
plt.show()



import numpy as np
import torch
import timeit
import matplotlib.pyplot as plt

def multiply_numpy(A, B):
    return np.dot(A, B)

def multiply_torch(A, B):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    A_torch = torch.tensor(A, dtype=torch.float32).to(device)
    B_torch = torch.tensor(B, dtype=torch.float32).to(device)
    return torch.matmul(A_torch, B_torch).cpu()

# Sizes of the matrices to test
sizes = [256, 512, 1024, 2048, 4096,2**13,2**14]
numpy_times = []
torch_times = []
number=10
for size in sizes:
    A = np.random.rand(size, size).astype(np.float32)
    B = np.random.rand(size, size).astype(np.float32)

    # Time NumPy
    numpy_duration = timeit.timeit('multiply_numpy(A, B)', globals=globals(), number=number)/number
    numpy_times.append(numpy_duration)

    # Time PyTorch with GPU
    torch_duration = timeit.timeit('multiply_torch(A, B)', globals=globals(), number=number)/number
    torch_times.append(torch_duration)

    print(f"Size: {size}x{size}")
    print("NumPy Duration:", numpy_duration)
    print("Torch GPU Duration:", torch_duration)
    if torch.cuda.is_available():
        print("PyTorch is using GPU.\n")
    else:
        print("PyTorch is not using GPU.\n")

# Plotting the results
plt.figure(figsize=(10, 5))
plt.plot(sizes, numpy_times, label='NumPy', marker='o')
plt.plot(sizes, torch_times, label='PyTorch GPU', marker='x')
plt.xlabel('Matrix Size')
plt.ylabel('Time (seconds)')
plt.title('Performance Comparison: NumPy vs. PyTorch GPU')
plt.yscale('log')  # Set the y-axis to logarithmic scale
plt.legend()
plt.grid(True, which="both", ls="--")  # Enable grid for major and minor ticks
plt.show()



# Example: Singular Value Decomposition
import numpy as np
import torch

size = 512
A = np.random.rand(size, size).astype(np.float32)
A_torch = torch.tensor(A, device='cuda')

def numpy_svd(A):
    return np.linalg.svd(A, full_matrices=False)

def torch_svd(A_torch):
    return torch.linalg.svd(A_torch, full_matrices=False).cpu()

# Timing and comparison code goes here



import timeit
number = 10
# Plain Python timing
python_time = timeit.timeit('matrix_multiply(A, B)', globals=globals(), number=number)
print("Average time per run for plain Python: {:.5f} seconds".format(python_time / number))

# NumPy timing
numpy_time = timeit.timeit('matrix_multiply_np(A_np, B_np)', globals=globals(), number=number)
print("Average time per run for NumPy: {:.5f} seconds".format(numpy_time / number))

# PyTorch timing
torch_time = timeit.timeit('matrix_multiply_torch(A_np, B_np)', globals=globals(), number=number)
print("Average time per run for PyTorch (GPU): {:.5f} seconds".format(torch_time / number))



def euclidean_distance_python(points):
    n = len(points)
    distances = [[0] * n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            if i != j:
                dx = points[i][0] - points[j][0]
                dy = points[i][1] - points[j][1]
                dz = points[i][2] - points[j][2]
                distances[i][j] = (dx**2 + dy**2 + dz**2)**0.5
    return distances



import numpy as np

def euclidean_distance_numpy(points):
    points = np.array(points)
    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]
    distances = np.linalg.norm(diff, axis=-1)
    return distances



import torch

def euclidean_distance_torch(points):
    points = torch.tensor(points).cuda()
    diff = points[:, None, :] - points[None, :, :]
    distances = torch.norm(diff, dim=-1)
    return distances.cpu().numpy()



import timeit

# Example points in 3D space
n=4000
points = np.random.rand(n, 3)  # 100 3D points

# Testing with timeit
number = 10  # Number of executions

python_time = timeit.timeit('euclidean_distance_python(points)', globals=globals(), number=number)
print("Average time per run for plain Python: {:.5f} seconds".format(python_time / number))

numpy_time = timeit.timeit('euclidean_distance_numpy(points)', globals=globals(), number=number)
print("Average time per run for NumPy: {:.5f} seconds".format(numpy_time / number))

#torch_time = timeit.timeit('euclidean_distance_torch(points)', globals=globals(), number=number)
#print("Average time per run for PyTorch (GPU): {:.5f} seconds".format(torch_time / number))



def euclidean_distance_python_optimized(points):
    n = len(points)
    distances = [[0] * n for _ in range(n)]  # Initialize a square matrix of zeroes
    for i in range(n):
        for j in range(i + 1, n):  # Start from the next point to avoid repetition
            dx = points[i][0] - points[j][0]
            dy = points[i][1] - points[j][1]
            dz = points[i][2] - points[j][2]
            dist = (dx**2 + dy**2 + dz**2)**0.5
            distances[i][j] = dist
            distances[j][i] = dist  # Use symmetry to fill the opposite element
    return distances



import timeit

# Example points in 3D space
n=4000
points = np.random.rand(n, 3)  # 100 3D points

# Timing the optimized function
number = 10  # Number of executions

optimized_python_time = timeit.timeit('euclidean_distance_python_optimized(points)', globals=globals(), number=number)
print("Average time per run for optimized plain Python: {:.5f} seconds".format(optimized_python_time / number))

