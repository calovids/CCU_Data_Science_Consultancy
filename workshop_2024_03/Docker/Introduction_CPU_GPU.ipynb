{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matrix_multiply(A, B):\n",
    "    n = len(A)\n",
    "    p = len(B[0])\n",
    "    m = len(B)\n",
    "    result = [[0] * p for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(p):\n",
    "            for k in range(m):\n",
    "                result[i][j] += A[i][k] * B[k][j]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matrix_multiply_np(A, B,Reps):\n",
    "    C=np.dot(A, B)\n",
    "    for i in range(Reps):\n",
    "        C=np.dot(A, C)\n",
    "    return C    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matrix_multiply_torch(A, B, Reps):\n",
    "    # Check if CUDA (GPU support) is available\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    A_torch = torch.tensor(A, dtype=torch.float64).to(device)\n",
    "    B_torch = torch.tensor(B, dtype=torch.float64).to(device)\n",
    "    C_torch = torch.matmul(A_torch, B_torch)  # Perform the first multiplication on the GPU\n",
    "\n",
    "    for i in range(Reps):  # Subtract 1 because the first multiplication is already done\n",
    "        C_torch = torch.matmul(A_torch, C_torch)  # Further multiplications on the GPU\n",
    "\n",
    "    C = C_torch.cpu().numpy()  # Only move the final result to CPU and convert to NumPy\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n",
      "Plain Python time: 20.980509281158447\n",
      "NumPy time: 0.06884336471557617\n",
      "PyTorch (GPU) time: 0.11136174201965332\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Generate random matrices\n",
    "n = 512\n",
    "Reps=20\n",
    "A = np.random.rand(n, n).astype(np.float64)\n",
    "B = np.random.rand(n, n).astype(np.float64)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device is:\",device)\n",
    "\n",
    "#Measure performance of plain Python\n",
    "start_time = time.time()\n",
    "result_python = matrix_multiply(A.tolist(), B.tolist())\n",
    "print(\"Plain Python time:\", time.time() - start_time)\n",
    "\n",
    "# Measure performance of NumPy\n",
    "start_time = time.time()\n",
    "result_np = matrix_multiply_np(A, B,Reps)\n",
    "print(\"NumPy time:\", time.time() - start_time)\n",
    "\n",
    "# Measure performance of PyTorch with GPU\n",
    "start_time = time.time()\n",
    "result_torch = matrix_multiply_torch(A, B,Reps)\n",
    "print(\"PyTorch (GPU) time:\", time.time() - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using timeit to properly benchmark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n",
      "numpy  1 0.0005832828499478637\n",
      "Torch  1 0.007143593749970023\n",
      "numpy  11 0.006958259450038895\n",
      "Torch  11 0.009766990400021314\n",
      "numpy  21 0.006901720899986685\n",
      "Torch  21 0.013511899199875188\n",
      "numpy  31 0.014352704500015533\n",
      "Torch  31 0.011602326049978729\n",
      "numpy  41 0.017692768649976643\n",
      "Torch  41 0.01348389485010557\n",
      "numpy  51 0.02943836619997455\n",
      "Torch  51 0.016737394099982338\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use the 'Agg' backend for file creation\n",
    "import torch\n",
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device is:\",device)\n",
    "\n",
    "n = 256\n",
    "A = np.random.rand(n, n).astype(np.float64)\n",
    "B = np.random.rand(n, n).astype(np.float64)\n",
    "reps_range = range(1, 61, 10)\n",
    "numpy_times = []\n",
    "torch_times = []\n",
    "number = 20\n",
    "\n",
    "for Reps in reps_range:\n",
    "    # Time NumPy\n",
    "    numpy_time = timeit.timeit('matrix_multiply_np(A, B, Reps)', globals=globals(), number=number)/number\n",
    "    print('numpy ',Reps, numpy_time)\n",
    "    numpy_times.append(numpy_time)\n",
    "\n",
    "    # Time PyTorch\n",
    "    torch_time = timeit.timeit('matrix_multiply_torch(A, B,  Reps)', globals=globals(), number=number)/number\n",
    "    print('Torch ',Reps, torch_time)\n",
    "    torch_times.append(torch_time)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reps_range, numpy_times, label='NumPy', marker='o')\n",
    "plt.plot(reps_range, torch_times, label='PyTorch', marker='x')\n",
    "plt.xlabel('Number of Repetitions (Reps)')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Performance Comparison: NumPy vs. PyTorch GPU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.draw()\n",
    "plt.savefig('reps_docker.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using more GPU intensive calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 256x256\n",
      "NumPy Duration: 0.0062711238861083984\n",
      "Torch GPU Duration: 0.03236246109008789\n",
      "Difference: 1.2789769243681803e-13 \n",
      "\n",
      "Size: 512x512\n",
      "NumPy Duration: 0.0126190185546875\n",
      "Torch GPU Duration: 0.023182153701782227\n",
      "Difference: 0.0 \n",
      "\n",
      "Size: 1024x1024\n",
      "NumPy Duration: 0.020261287689208984\n",
      "Torch GPU Duration: 0.049964189529418945\n",
      "Difference: 1.0800249583553523e-12 \n",
      "\n",
      "Size: 2048x2048\n",
      "NumPy Duration: 0.12004399299621582\n",
      "Torch GPU Duration: 0.21376752853393555\n",
      "Difference: 3.069544618483633e-12 \n",
      "\n",
      "Size: 4096x4096\n",
      "NumPy Duration: 1.022993803024292\n",
      "Torch GPU Duration: 0.9930586814880371\n",
      "Difference: 8.86757334228605e-12 \n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 5.61 GiB total capacity; 4.01 GiB already allocated; 1.48 GiB free; 4.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39213/1962886778.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Time PyTorch with GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mCtorch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiply_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mtorch_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtorch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_duration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_39213/1962886778.py\u001b[0m in \u001b[0;36mmultiply_torch\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mA_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mB_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Sizes of the matrices to test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 5.61 GiB total capacity; 4.01 GiB already allocated; 1.48 GiB free; 4.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def multiply_numpy(A, B):\n",
    "    return np.dot(A, B)\n",
    "\n",
    "def multiply_torch(A, B):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    A_torch = torch.tensor(A, dtype=torch.float64).to(device)\n",
    "    B_torch = torch.tensor(B, dtype=torch.float64).to(device)\n",
    "    return torch.matmul(A_torch, B_torch).cpu().numpy()\n",
    "\n",
    "# Sizes of the matrices to test\n",
    "sizes = [256, 512, 2**10, 2**11, 2**12,2**13]\n",
    "numpy_times = []\n",
    "torch_times = []\n",
    "\n",
    "\n",
    "for size in sizes:\n",
    "    A = np.random.rand(size, size).astype(np.float64)\n",
    "    B = np.random.rand(size, size).astype(np.float64)\n",
    "\n",
    "    # Time NumPy\n",
    "    start_time = time.time()\n",
    "    Cnp=multiply_numpy(A, B)\n",
    "    numpy_duration = time.time() - start_time\n",
    "    numpy_times.append(numpy_duration)\n",
    "\n",
    "    # Time PyTorch with GPU\n",
    "    start_time = time.time()\n",
    "    Ctorch=multiply_torch(A, B)\n",
    "    torch_duration = time.time() - start_time\n",
    "    torch_times.append(torch_duration)\n",
    "\n",
    "    print(f\"Size: {size}x{size}\")\n",
    "    print(\"NumPy Duration:\", numpy_duration)\n",
    "    print(\"Torch GPU Duration:\", torch_duration)\n",
    "    print(\"Difference:\",np.max(np.abs(Cnp-Ctorch)),'\\n')    \n",
    "    \n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sizes, numpy_times, label='NumPy', marker='o')\n",
    "plt.plot(sizes, torch_times, label='PyTorch GPU', marker='x')\n",
    "plt.xlabel('Matrix Size')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Performance Comparison: NumPy vs. PyTorch GPU')\n",
    "plt.yscale('log')  # Set the y-axis to logarithmic scale\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")  # Enable grid for major and minor ticks\n",
    "plt.draw()\n",
    "plt.savefig('sizes_docker.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different GPU optimized functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Duration: 1.8802527305000694\n",
      "Torch GPU Duration: 0.9778659745999903\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import timeit\n",
    "\n",
    "# Example image loading and conversion to numpy\n",
    "image = Image.open('Kinkade.jpg').convert('RGB')  # Ensure image is in RGB\n",
    "image_np = np.array(image).astype(np.float32)  # Convert to float for better handling in PyTorch\n",
    "image_torch = torch.tensor(image_np.transpose(2, 0, 1)).unsqueeze(0).to('cuda')  # Convert to Tensor and move to GPU\n",
    "\n",
    "# Define the target size as a tuple (width, height)\n",
    "size = (512*15, 512*15)  # Define the size as a tuple\n",
    "\n",
    "def numpy_resize(image_np, size):\n",
    "    return np.array(Image.fromarray(image_np.astype(np.uint8)).resize(size))\n",
    "\n",
    "def torch_resize(image_torch, size):\n",
    "    # size needs to be (height, width) for PyTorch, reverse the tuple\n",
    "    torch_size = (size[1], size[0])\n",
    "    return torch.nn.functional.interpolate(image_torch, size=torch_size, mode='bilinear', align_corners=False).cpu()\n",
    "\n",
    "# Timing the functions using timeit\n",
    "number = 10  # Number of iterations for timing\n",
    "\n",
    "# Time NumPy\n",
    "numpy_time = timeit.timeit('numpy_resize(image_np, size)', globals=globals(), number=number) / number\n",
    "\n",
    "# Time PyTorch\n",
    "torch_time = timeit.timeit('torch_resize(image_torch, size)', globals=globals(), number=number) / number\n",
    "\n",
    "print(\"NumPy Duration:\", numpy_time)\n",
    "print(\"Torch GPU Duration:\", torch_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions not optimized for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy time: 0.29374332609950216\n",
      "Torch time: 1.0775124672996754\n"
     ]
    }
   ],
   "source": [
    "# Creating a large array and summing it\n",
    "import timeit\n",
    "import numpy as np\n",
    "import torch\n",
    "large_data = np.random.rand(10**8)  # 10 million elements\n",
    "\n",
    "\n",
    "def sum_numpy(data):\n",
    "    return np.sum(data)\n",
    "\n",
    "def sum_torch(data):\n",
    "    tensor = torch.tensor(data).cuda()\n",
    "    return torch.sum(tensor).cpu()\n",
    "\n",
    "number=10\n",
    "# Timing the sum operations\n",
    "numpy_time = timeit.timeit('sum_numpy(large_data)', globals=globals(), number=number) / number\n",
    "\n",
    "\n",
    "torch_time = timeit.timeit('sum_torch(large_data)', globals=globals(), number=number) / number\n",
    "\n",
    "\n",
    "print(\"NumPy time:\", numpy_time)\n",
    "print(\"Torch time:\", torch_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
